<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GPU Fundamentals & LLM Inference Mental Models | Akhil Shekkari</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism.min.css">
    <link rel="stylesheet" href="../css/styles.css">
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js"></script>
</head>
<body>
    <a href="../blogs.html" class="back-link">&larr; Back to Blog</a>

    <article>
        <h1>GPU Fundamentals &amp; LLM Inference Mental Models</h1>

        <p class="blog-meta">Summary &middot; 20 min read</p>
        <hr class="section-break">

        <p>
            Research Engineer interviews at leading AI labs test your ability to reason about inference performance from first principles.
            You will be expected to estimate whether a model fits on a given GPU, explain why autoregressive decoding is slow,
            identify bottlenecks (memory bandwidth vs. compute) for a given workload, and propose optimizations that target
            the actual bottleneck.
        </p>
        <p>
            This article builds that foundation. We cover GPU architecture, the roofline model, memory estimation,
            arithmetic intensity analysis, and latency estimation , everything you need to develop strong intuition
            about what makes LLM inference fast or slow.
        </p>

        <hr class="section-break">

        <!-- ============================================================ -->
        <!-- SECTION 1: GPU ARCHITECTURE -->
        <!-- ============================================================ -->

        <h2>GPU Architecture Fundamentals</h2>

        <h3>NVIDIA A100 Architecture</h3>

        <p>
            The A100 is the workhorse GPU for LLM inference and training. Understanding its architecture is essential
            for reasoning about performance.
        </p>

        <p class="key-formula">
            The A100-80GB packs <strong>108 SMs</strong>, each with 64 CUDA cores and 4 Tensor cores, connected via <strong>40 MB L2</strong> to <strong>80 GB HBM2e</strong> at 2 TB/s. Fast on-chip SRAM (192 KB per SM) sits next to each SM; the size and speed gap between SRAM and HBM is what makes memory bandwidth the bottleneck.
        </p>

        <h3>Key Numbers to Memorize</h3>

        <div class="spec-cards">
            <div class="spec-card"><span class="term">Streaming Multiprocessors</span><div class="value">108</div><div class="note">Each SM is an independent processor</div></div>
            <div class="spec-card"><span class="term">CUDA Cores</span><div class="value">6912 (64/SM)</div><div class="note">Scalar FP ops</div></div>
            <div class="spec-card"><span class="term">Tensor Cores</span><div class="value">432 (4/SM)</div><div class="note">Matrix multiply (FP16/BF16/INT8)</div></div>
            <div class="spec-card"><span class="term">HBM2e Capacity</span><div class="value">80 GB</div><div class="note">Main VRAM</div></div>
            <div class="spec-card"><span class="term">HBM Bandwidth</span><div class="value">2.0 TB/s</div><div class="note">Data rate from HBM</div></div>
            <div class="spec-card"><span class="term">L2 Cache</span><div class="value">40 MB</div><div class="note">Shared across SMs</div></div>
            <div class="spec-card"><span class="term">L1/Shared per SM</span><div class="value">192 KB</div><div class="note">Fast on-chip</div></div>
            <div class="spec-card"><span class="term">Total SRAM</span><div class="value">~20.25 MB</div><div class="note">108 &times; 192 KB</div></div>
            <div class="spec-card"><span class="term">FP16 Throughput</span><div class="value">312 TFLOPS</div><div class="note">Peak, Tensor Cores</div></div>
            <div class="spec-card"><span class="term">FP32 Throughput</span><div class="value">156 TFLOPS</div><div class="note">Peak FP32</div></div>
        </div>

        <h3>Execution Model: Warps and Thread Blocks</h3>

        <ul>
            <li><strong>Kernel launch</strong> &rarr; a grid of thread blocks is scheduled onto SMs.</li>
            <li><strong>Each block</strong> has up to 1024 threads, split into warps.</li>
            <li><strong>Each warp</strong> = 32 threads executing in lockstep (SIMT); they all run the same instruction at once.</li>
        </ul>

        <p>
            The <strong>warp</strong> is the fundamental unit of execution. All 32 threads in a warp execute the same
            instruction at the same time. If threads diverge (different <code>if</code> branches), both paths must be
            executed serially , this is called <strong>warp divergence</strong> and it wastes cycles.
        </p>

        <h3>SRAM vs. HBM: The Crucial Ratio</h3>

        <ul>
            <li><strong>Total SRAM</strong>: ~20 MB (fast, on-chip, ~30 cycle latency)</li>
            <li><strong>Total HBM</strong>: 80 GB (slow, off-chip, ~400 cycle latency)</li>
            <li><strong>Ratio</strong>: HBM is ~4000x larger but ~13x slower</li>
        </ul>

        <p>
            This mismatch is the fundamental reason why <strong>memory bandwidth is the bottleneck</strong> for most LLM
            inference workloads. The model weights live in HBM, and for each token generated during decode, we must read
            ALL weights from HBM through a limited bandwidth pipe.
        </p>

        <hr class="section-break">

        <!-- ============================================================ -->
        <!-- MEMORY HIERARCHY -->
        <!-- ============================================================ -->

        <h3>Memory Hierarchy Latency</h3>

        <div class="memory-stack">
            <div class="memory-tier"><span class="name">Registers</span><span class="latency">~1 cycle</span><span class="size">64K 32-bit regs/SM</span></div>
            <div class="memory-tier"><span class="name">L1 / SRAM</span><span class="latency">~30 cycles</span><span class="size">192 KB/SM = 20 MB total, ~19 TB/s aggregate</span></div>
            <div class="memory-tier"><span class="name">L2 Cache</span><span class="latency">~200 cycles</span><span class="size">40 MB, ~5–6 TB/s</span></div>
            <div class="memory-tier"><span class="name">HBM</span><span class="latency">~400 cycles</span><span class="size">80 GB, 2.0 TB/s</span></div>
        </div>

        <h3>Why This Matters for LLM Inference</h3>

        <p>
            During <strong>autoregressive decode</strong>, each generated token requires reading the full model weights from HBM:
        </p>

        <ul>
            <li><strong>Llama-7B at FP16</strong>: 14 GB of weights</li>
            <li><strong>At 2 TB/s bandwidth</strong>: Takes 14 GB / 2 TB/s = <strong>7 ms</strong> just to read the weights</li>
            <li><strong>Computation</strong>: ~14 GFLOP per token, which at 312 TFLOPS takes only <strong>0.045 ms</strong></li>
        </ul>

        <p>
            The weight-loading time is <strong>150x larger</strong> than the compute time. This is why decode is
            memory-bandwidth-bound, and why all the major inference optimizations (quantization, KV caching,
            speculative decoding, batching) ultimately aim to reduce the bytes-per-useful-FLOP ratio.
        </p>

        <div class="note-block">
            <strong>Rule of thumb:</strong> If you can do the arithmetic faster than you can load the data, you are
            <strong>memory-bound</strong>. For single-token decode, this is almost always the case.
        </div>

        <hr class="section-break">

        <!-- ============================================================ -->
        <!-- SECTION 2: ROOFLINE MODEL -->
        <!-- ============================================================ -->

        <h2>The Roofline Model</h2>

        <p>
            The <strong>roofline model</strong> is the single most important mental model for understanding inference
            performance. It tells you whether a workload is limited by:
        </p>
        <ol>
            <li><strong>Memory bandwidth</strong> (loading data from HBM) , left side of the plot</li>
            <li><strong>Compute throughput</strong> (doing arithmetic) , right side / top of the plot</li>
        </ol>

        <h3>Arithmetic Intensity</h3>

        <div class="key-formula">
            <strong>AI = FLOPs / Bytes Accessed</strong> (FLOPs per byte). Low AI (&lt; ridge point) &rarr; memory-bound; high AI (&gt; ridge point) &rarr; compute-bound.
        </div>

        <h3>Ridge Point</h3>

        <p>
            The <strong>ridge point</strong> is the arithmetic intensity at which the compute and memory roofs intersect:
        </p>

        <div class="key-formula">
            Ridge = Peak Compute / Peak Bandwidth. A100-80GB: 312 TFLOP/s ÷ 2.0 TB/s = <span class="result">156 FLOPs/byte</span>.
        </div>

        <h3>Attainable Performance</h3>

        <div class="key-formula">
            Attainable = min(Peak Compute, AI &times; Bandwidth). If AI &lt; 156: memory-limited (AI &times; 2 TB/s). If AI &ge; 156: compute-limited (312 TFLOPS).
        </div>

        <div class="diagram-container">
            <img src="../images/gpu-fundamentals/roofline.png" alt="A100-80GB Roofline Model showing Prefill vs Decode positions" class="diagram">
            <div class="diagram-caption">Figure 1: The A100-80GB Roofline Model. Decode (batch=1) sits at AI=1, achieving only 0.6% of peak compute. Prefill (seq=2048) sits near the ridge point, approaching full compute utilization. The red region is memory-bound; the green region is compute-bound.</div>
        </div>

        <h3>The Key Insight: Prefill is Compute-Bound, Decode is Memory-Bound</h3>

        <p>This single insight explains nearly every optimization in modern LLM serving.</p>

        <div class="phase-cards">
            <div class="phase-card prefill">
                <h4>Prefill</h4>
                <div class="what">Process entire prompt in one shot. Matrix-matrix multiply: (seq_len &times; d_model) @ (d_model &times; d_model).</div>
                <div class="ai">AI scales with seq_len; for seq=2048, AI ~ 150.</div>
                <div class="bottleneck">Compute-bound: limited by TFLOPS</div>
            </div>
            <div class="phase-card decode">
                <h4>Decode</h4>
                <div class="what">Generate one token at a time. Matrix-vector multiply: (1 &times; d_model) @ (d_model &times; d_model).</div>
                <div class="ai">AI ~ 1 (each weight byte used once per batch item).</div>
                <div class="bottleneck">Memory-bound: limited by HBM bandwidth</div>
            </div>
        </div>

        <p>
            <strong>Why is decode so memory-inefficient?</strong> During decode, we generate ONE token. This means we
            do a matrix-vector product: the weight matrix has <code>d_model &times; d_model</code> elements, but the
            input vector has only <code>d_model</code> elements. Each weight is loaded from HBM but used for only
            <strong>one multiply-add</strong> (2 FLOPs / 2 bytes at FP16 = AI of 1).
        </p>

        <p>
            <strong>Why is prefill efficient?</strong> During prefill, the input is a matrix of shape
            <code>(seq_len &times; d_model)</code>. The same weight matrix (loaded once from HBM) is multiplied
            against <code>seq_len</code> vectors. The weight bytes are <strong>amortized</strong> across
            <code>seq_len</code> tokens, giving AI ~ <code>seq_len</code>.
        </p>

        <h3>Optimization Implications</h3>

        <div class="opt-cards">
            <div class="opt-card"><strong>Quantization</strong> (INT4/INT8) <span class="target">Decode</span>, fewer bytes from HBM per weight</div>
            <div class="opt-card"><strong>Batching</strong> <span class="target">Decode</span>, amortize weight load across B sequences (AI &times; B)</div>
            <div class="opt-card"><strong>FlashAttention</strong> <span class="target">Prefill</span>, tile attention into SRAM, fewer HBM reads</div>
            <div class="opt-card"><strong>Speculative Decoding</strong> <span class="target">Decode</span>, verify multiple tokens in one pass</div>
            <div class="opt-card"><strong>KV Cache</strong> <span class="target">Both</span>, avoid recomputing attention for past tokens</div>
            <div class="opt-card"><strong>Tensor Parallelism</strong> <span class="target">Both</span>, split weights across GPUs</div>
        </div>

        <hr class="section-break">

        <!-- ============================================================ -->
        <!-- SECTION 3: MEMORY ESTIMATION -->
        <!-- ============================================================ -->

        <h2>Memory Estimation</h2>

        <p>
            Understanding where GPU memory goes during inference is critical for capacity planning. Let's break down
            the memory components for Llama-7B.
        </p>

        <h3>VRAM Breakdown: Llama-7B at FP16</h3>

        <div class="diagram-container">
            <img src="../images/gpu-fundamentals/vram_breakdown.png" alt="Pie chart showing VRAM breakdown for Llama-7B at FP16" class="diagram">
            <div class="diagram-caption">Figure 2: Llama-7B memory breakdown (FP16, batch=1, seq=2048). Weights 87.8% (13.04 GB), KV cache 6.7% (1.00 GB), activations 3.4% (0.51 GB), CUDA overhead 2.1% (0.31 GB). Total ~14.86 GB.</div>
        </div>

        <div class="note-block">
            <strong>Key observations:</strong>
            <ul>
                <li>At batch=1, model weights dominate total memory (~90%+)</li>
                <li>KV cache is small at batch=1 but grows linearly with batch size and sequence length</li>
                <li>Activations during inference are negligible (only one layer active at a time)</li>
                <li>CUDA overhead is a fixed ~0.5 GB cost</li>
            </ul>
        </div>

        <h3>Does Llama-70B Fit on A100-80GB?</h3>

        <div class="opt-cards">
            <div class="opt-card"><strong>FP16</strong> 130 GB total &rarr; <span class="target">No</span> (exceeds 80 GB; need 2 GPUs / tensor parallelism)</div>
            <div class="opt-card"><strong>INT8</strong> 66.9 GB total &rarr; <span class="target">Yes</span> (~13 GB headroom)</div>
            <div class="opt-card"><strong>INT4</strong> 34.4 GB total &rarr; <span class="target">Yes</span> (~46 GB headroom)</div>
        </div>

        <div class="note-block">
            <strong>Interview insight:</strong> Llama-70B at FP16 needs ~130 GB = 2&times; A100-80GB.
            Quantizing to INT4 brings it down to ~35 GB, fitting on a single GPU.
            This is why quantization is so important for deployment.
        </div>

        <h3>When Does KV Cache Dominate?</h3>

        <p>
            At batch=1, weights dominate. But at production batch sizes, KV cache quickly overtakes weights.
            This plot shows the crossover point for Llama-7B.
        </p>

        <div class="diagram-container">
            <img src="../images/gpu-fundamentals/kv_cache_crossover.png" alt="KV Cache vs Weights memory as batch size increases" class="diagram">
            <div class="diagram-caption">Figure 3: Llama-7B FP16 (seq_len=2048), KV cache vs weights vs batch size. Weights constant at ~13 GB; KV cache grows linearly. KV cache equals weights at batch=14. Max batch=50 on A100-80GB before OOM. KV cache management (PagedAttention, GQA) is critical at scale.</div>
        </div>

        <h3>Memory Across Model Sizes and Precisions</h3>

        <div class="model-cards">
            <div class="model-card"><span class="name">Llama-7B</span><span class="dtype">FP16</span><div class="mem">Weights 13.0 GB · KV 0.50 GB · Total 14.0 GB</div><div class="fits yes">✓ Fits A100</div></div>
            <div class="model-card"><span class="name">Llama-7B</span><span class="dtype">INT4</span><div class="mem">Weights 3.3 GB · KV 0.50 GB · Total 4.3 GB</div><div class="fits yes">✓ Fits A100</div></div>
            <div class="model-card"><span class="name">Llama-13B</span><span class="dtype">FP16</span><div class="mem">Weights 24.2 GB · KV 0.78 GB · Total 25.5 GB</div><div class="fits yes">✓ Fits A100</div></div>
            <div class="model-card"><span class="name">Llama-13B</span><span class="dtype">INT4</span><div class="mem">Weights 6.1 GB · KV 0.78 GB · Total 7.4 GB</div><div class="fits yes">✓ Fits A100</div></div>
            <div class="model-card"><span class="name">Llama-70B</span><span class="dtype">FP16</span><div class="mem">Weights 130 GB · KV 1.25 GB · Total 131.9 GB</div><div class="fits no">✗ 2 GPUs</div></div>
            <div class="model-card"><span class="name">Llama-70B</span><span class="dtype">INT4</span><div class="mem">Weights 32.5 GB · KV 1.25 GB · Total 34.4 GB</div><div class="fits yes">✓ Fits A100</div></div>
        </div>

        <p>
            <strong>Key observations:</strong>
        </p>
        <ol>
            <li>Weights scale linearly with parameter count and inversely with quantization.</li>
            <li>INT4 gives a 4x reduction in weight memory vs FP16, making 70B feasible on one GPU.</li>
            <li>KV cache for 70B is smaller than expected thanks to GQA (8 KV heads vs 64 query heads).</li>
            <li>At batch=1, weights dominate. At production batch sizes (32+), KV cache dominates.</li>
        </ol>

        <hr class="section-break">

        <!-- ============================================================ -->
        <!-- SECTION 4: ARITHMETIC INTENSITY -->
        <!-- ============================================================ -->

        <h2>Arithmetic Intensity Analysis</h2>

        <p>
            Let's see how arithmetic intensity changes with different operating conditions, and how that
            determines whether we're memory-bound or compute-bound.
        </p>

        <h3>Decode AI vs Batch Size</h3>

        <p>
            Decode at batch=1 has an arithmetic intensity of just 1 , only 0.6% of the ridge point.
            Even at batch=64, we're still deeply memory-bound. It takes a batch size of ~156 to fully saturate
            the A100's compute capability during decode.
        </p>

        <div class="diagram-container">
            <img src="../images/gpu-fundamentals/Decode_AI_vs_batch_size.png" alt="Decode arithmetic intensity vs batch size" class="diagram">
            <div class="diagram-caption">Figure 4: Decode arithmetic intensity grows linearly with batch size. The blue dashed line marks the A100 ridge point at 156 FLOPs/byte. Below this line, the GPU's compute cores are underutilized , adding more sequences to the batch is essentially "free" in terms of latency, because we're bottlenecked on memory bandwidth anyway.</div>
        </div>

        <h3>Prefill AI vs Sequence Length</h3>

        <p>
            Prefill crosses the ridge point at relatively short sequence lengths (~300-400 tokens).
            For typical prompt lengths (1K+ tokens), prefill is solidly compute-bound , the GPU cores
            become the bottleneck, not memory bandwidth.
        </p>

        <div class="diagram-container">
            <img src="../images/gpu-fundamentals/prefil_vs_seq.png" alt="Prefill arithmetic intensity vs sequence length" class="diagram">
            <div class="diagram-caption">Figure 5: Prefill arithmetic intensity vs sequence length (Llama-7B, FP16). Crosses A100 ridge (156 FLOPs/byte) at seq_len ~192; below that, prefill is memory-bound; above, compute-bound. For short prompts, latency is dominated by weight loading; for long prompts, by computation.</div>
        </div>

        <hr class="section-break">

        <!-- ============================================================ -->
        <!-- SECTION 5: TIME ESTIMATES -->
        <!-- ============================================================ -->

        <h2>Time Estimates</h2>

        <p>
            Using the roofline model, we can estimate latencies for prefill (TTFT = time to first token)
            and decode (per-token generation time). These back-of-the-envelope calculations are exactly
            the kind of reasoning expected in interviews.
        </p>

        <h3>Prefill Latency (TTFT)</h3>

        <p>
            Prefill latency scales roughly linearly with prompt length. For Llama-7B on A100,
            a 2048-token prompt takes about 9 ms , fast enough to be imperceptible.
        </p>

        <div class="diagram-container">
            <img src="../images/gpu-fundamentals/ttft_vs_prompt_length.png" alt="Time to first token vs prompt length for different model sizes" class="diagram">
            <div class="diagram-caption">Figure 6: Prefill latency (TTFT) vs prompt length for Llama-7B, 13B, and 70B on A100 (FP16, batch=1). Horizontal reference lines mark 100ms (imperceptible), 500ms (noticeable), and 1000ms (slow). Larger models have proportionally higher TTFT. For Llama-70B, prompts beyond 4K tokens push TTFT past 1 second on a single A100.</div>
        </div>

        <h3>Decode Latency and Throughput</h3>

        <p class="key-formula">
            Decode time stays ~7 ms per step while memory-bound (batch &lt; ~164); total tokens/sec scales with batch. Past the ridge, latency rises and throughput plateaus (~22K tokens/sec). See the diagram below.
        </p>

        <p>
            The key insight: <strong>decode time per step stays nearly constant as batch size increases</strong>
            (while memory-bound). This means total throughput scales linearly with batch size , for free!
            This is the fundamental insight behind continuous batching.
        </p>

        <div class="diagram-container">
            <img src="../images/gpu-fundamentals/decode_latency_throughput.png" alt="Decode latency and throughput vs batch size" class="diagram">
            <div class="diagram-caption">Figure 7: Left: Llama-7B decode latency per step stays flat (~7 ms) while memory-bound, then rises after the inflection at batch=164 (transition to compute-bound). Right: Throughput scales linearly with batch size in the memory-bound regime (&quot;free&quot; batching), then plateaus at ~22K tokens/sec. This is why production serving uses continuous batching.</div>
        </div>

        <div class="note-block">
            <strong>Key observations:</strong>
            <ul>
                <li>Decode time per step stays nearly constant as batch size increases (while memory-bound)</li>
                <li>Total throughput (tokens/sec) scales linearly with batch size , for free</li>
                <li>This is the fundamental insight behind continuous batching (vLLM, TGI)</li>
                <li>Per-request latency stays the same , everyone gets the same speed, but the server handles more requests</li>
            </ul>
        </div>

        <hr class="section-break">

        <!-- ============================================================ -->
        <!-- SECTION 6: KEY TAKEAWAYS -->
        <!-- ============================================================ -->

        <h2>Key Takeaways</h2>

        <h3>5 Things You Should Know</h3>

        <ol>
            <li>
                <strong>"Autoregressive decode is memory-bandwidth-bound, not compute-bound."</strong>
                Each decode step requires reading all model weights from HBM to generate a single token.
                The arithmetic intensity is ~1 FLOP/byte (at batch=1, FP16), far below the A100's ridge point of 156.
                This means the Tensor Cores sit idle ~99% of the time during decode.
            </li>
            <li>
                <strong>"Prefill is compute-bound for reasonable sequence lengths."</strong>
                During prefill, the same weights are reused across all tokens in the prompt, giving arithmetic
                intensity that scales with <code>seq_len</code>. For seq_len > ~300 on A100, prefill becomes compute-bound.
            </li>
            <li>
                <strong>"Batching is the primary lever for decode throughput."</strong>
                Since decode is memory-bound, adding more sequences to a batch reuses the same weight data already
                being streamed from HBM. The per-step latency barely changes, but throughput scales linearly.
                This is why continuous batching (vLLM, TGI) is transformative.
            </li>
            <li>
                <strong>"KV cache memory grows as O(batch_size &times; seq_len &times; num_layers &times; d_head &times; num_kv_heads)."</strong>
                At production batch sizes (32+), KV cache can dominate total GPU memory. This is why PagedAttention,
                GQA (grouped-query attention), and KV cache quantization are critical.
            </li>
            <li>
                <strong>"Quantization helps decode more than prefill."</strong>
                Since decode is memory-bound, reducing the number of bytes per weight (FP16 &rarr; INT4 = 4x fewer bytes)
                directly translates to ~4x faster decode. For prefill (compute-bound), quantization helps less unless
                it also reduces the compute cost.
            </li>
        </ol>

        <hr class="section-break">

        <!-- ============================================================ -->
        <!-- APPENDIX: FORMULAS -->
        <!-- ============================================================ -->

        <h2>Quick-Reference Formulas</h2>

        <p>Keep these formulas in your head for back-of-the-envelope calculations:</p>

        <div class="spec-cards">
            <div class="spec-card"><span class="term">Weight Memory</span><div class="value">params &times; bytes/element</div><div class="note">Llama-7B FP16: 14 GB · 70B INT4: 35 GB</div></div>
            <div class="spec-card"><span class="term">KV Cache</span><div class="value">batch &times; seq &times; layers &times; (2 &times; kv_heads &times; head_dim &times; bytes)</div><div class="note">Per token per layer, then sum over layers</div></div>
            <div class="spec-card"><span class="term">Decode AI</span><div class="value">2 &times; B / bytes_per_element</div><div class="note">B=1 &rarr; 1; B=32 &rarr; 32. Prefill AI ~ seq_len.</div></div>
            <div class="spec-card"><span class="term">Decode time</span><div class="value">Weight Memory / HBM BW</div><div class="note">Llama-7B FP16 on A100: 14 GB / 2 TB/s = <span class="result">7 ms</span></div></div>
            <div class="spec-card"><span class="term">Ridge</span><div class="value">Peak FLOPS / Peak BW</div><div class="note">A100: 156 · H100: 296 FLOPs/byte</div></div>
        </div>

    </article>

    <footer>
        <p>&copy; 2025 Akhil Shekkari &middot; <a href="https://github.com/shekkari1999">GitHub</a> &middot; <a href="https://www.linkedin.com/in/akhilshekkari/">LinkedIn</a></p>
    </footer>

    <script>Prism.highlightAll();</script>
</body>
</html>
