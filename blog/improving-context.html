<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Improving Context: Reasoning & Long Context in LLMs | Akhil Shekkari</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism.min.css">
    <link rel="stylesheet" href="../css/styles.css">
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js"></script>
    <script>
        window.addEventListener('load', function() {
            document.querySelectorAll('pre, code').forEach(el => {
                el.style.fontSize = '11px';
                el.style.fontFamily = 'Consolas, Monaco, "Courier New", monospace';
            });
        });
    </script>
</head>
<body>
    <a href="../blogs.html" class="back-link">&larr; Back to Blog</a>

    <article>
        <h1>Improving Context: Reasoning & Long Context in LLMs</h1>

        <p class="blog-meta">Feb 6th, 2026 &middot; 25 min read</p>
        <hr class="section-break">

        <h2>1. Reasoning Over Long-Horizon Tasks</h2>

        <p>
            <strong>Long-horizon tasks</strong> are tasks that require many sequential steps, decisions, or
            intermediate sub-goals before reaching a final outcome. "Reasoning over" them means the model must
            plan, track state, and make coherent decisions across that entire chain.
        </p>

        <h3>Concrete Examples</h3>

        <div class="quantization-table">
            <table>
                <thead>
                    <tr>
                        <th>Domain</th>
                        <th>Short-Horizon</th>
                        <th>Long-Horizon</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Coding</td>
                        <td>"Fix this typo"</td>
                        <td>"Build a REST API with auth, DB, tests, and deploy it"</td>
                    </tr>
                    <tr>
                        <td>Math</td>
                        <td>"What is 2+3?"</td>
                        <td>"Prove this theorem using 15 intermediate lemmas"</td>
                    </tr>
                    <tr>
                        <td>Agents</td>
                        <td>"Search the web"</td>
                        <td>"Research a topic, synthesize findings, write a report, iterate on feedback"</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <h3>Why It's Hard for LLMs</h3>

        <ol>
            <li><strong>Error compounding</strong> &mdash; A small mistake in step 3 of 20 derails everything downstream. Errors accumulate multiplicatively.</li>
            <li><strong>State tracking</strong> &mdash; The model must remember what it has done, what remains, and what intermediate results it produced, across a long context.</li>
            <li><strong>Planning under uncertainty</strong> &mdash; Early decisions constrain later options. The model needs to reason about consequences several steps ahead.</li>
            <li><strong>Credit assignment</strong> &mdash; When the final answer is wrong, it's hard to identify <em>which</em> step failed.</li>
        </ol>

        <h3>How Models Address This</h3>

        <ul>
            <li><strong>Chain-of-thought (CoT)</strong> &mdash; Explicit step-by-step reasoning reduces the per-step difficulty.</li>
            <li><strong>Tree/graph search (e.g., ToT, GoT)</strong> &mdash; Explore multiple reasoning paths, backtrack when stuck.</li>
            <li><strong>Reinforcement learning (GRPO, PPO on reasoning traces)</strong> &mdash; Train models to get reward for correct <em>final</em> answers, forcing them to learn robust multi-step strategies. This is what DeepSeek-R1 and OpenAI's o1/o3 do.</li>
            <li><strong>Process reward models (PRMs)</strong> &mdash; Give reward at <em>each step</em>, not just the final answer, so the model learns which intermediate steps are good.</li>
            <li><strong>Decomposition</strong> &mdash; Break the long task into sub-tasks, solve each, then compose.</li>
        </ul>

        <h3>Why This Matters</h3>

        <ul>
            <li>Fine-tuning a model with <strong>outcome-based RL</strong> (like GRPO) teaches it to solve long-horizon tasks by trial and error over reasoning traces.</li>
            <li><strong>Distillation</strong> from a strong reasoning model (e.g., DeepSeek-R1 into a smaller model) transfers long-horizon reasoning ability cheaply.</li>
            <li>The key insight: <strong>supervised fine-tuning alone struggles with long-horizon reasoning</strong> because it only imitates. RL lets the model <em>discover</em> better strategies through exploration.</li>
        </ul>

        <div class="note-block">
            <strong>Key Insight:</strong> This is exactly the frontier of "reasoning models" right now &mdash; the gap
            between models that can chain 5 steps vs. 50 steps reliably.
        </div>

        <hr class="section-break">

        <h2>2. How Context Length Is Decided &mdash; Positional Encodings, Attention Cost & KV Cache</h2>

        <p>
            Context length is not a single knob. It's jointly determined by three interacting constraints.
        </p>

        <h3>Constraint 1: Positional Encoding Scheme</h3>

        <p>
            Transformers have no inherent notion of order &mdash; self-attention is permutation-invariant.
            Positional encodings inject order. The scheme you pick dictates the ceiling.
        </p>

        <div class="quantization-table">
            <table>
                <thead>
                    <tr>
                        <th>Scheme</th>
                        <th>How It Works</th>
                        <th>Limit</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Learned Absolute</strong> (original GPT)</td>
                        <td>Learns an embedding vector for positions 0..N-1</td>
                        <td>Hard ceiling at N. Position 4097 literally doesn't exist.</td>
                    </tr>
                    <tr>
                        <td><strong>Sinusoidal</strong> (original Transformer)</td>
                        <td>Fixed sin/cos at different frequencies</td>
                        <td>Theoretically infinite, but untested positions degrade</td>
                    </tr>
                    <tr>
                        <td><strong>RoPE</strong> (LLaMA, Qwen, most modern LLMs)</td>
                        <td>Encodes position as rotation angle in embedding space</td>
                        <td>Theoretically extrapolable, but rotations at unseen angles are out-of-distribution</td>
                    </tr>
                    <tr>
                        <td><strong>ALiBi</strong> (BLOOM)</td>
                        <td>Adds linear bias to attention scores based on distance</td>
                        <td>Better extrapolation, but penalizes long-range attention by design</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <h3>RoPE &mdash; The Dominant Approach (and Its Limitation)</h3>

        <p>
            Most modern models use <strong>Rotary Position Embedding (RoPE)</strong>. For each dimension pair (i),
            the rotation angle at position m is:
        </p>

        <div class="code-example">
&theta;_i(m) = m &times; base^(-2i/d)

where base = 10000 (typically), d = head dimension</div>

        <p>
            <strong>The problem:</strong> If you train on positions 0 to 4096, the model has only ever seen
            rotation angles in that range. Position 8000 produces rotations the model has never encountered. It's
            like teaching someone a 12-hour clock then asking them to read a 24-hour clock &mdash; the mechanism
            is the same but the values are foreign. This is a <strong>distribution shift</strong>, and the model's
            output degrades unpredictably.
        </p>

        <h3>Constraint 2: Attention's Quadratic Cost</h3>

        <p>
            Self-attention computes pairwise scores between <strong>every</strong> token pair:
        </p>

        <div class="code-example">
Attention(Q, K, V) = softmax(QK^T / &radic;d_k) V

QK^T matrix is n &times; n where n = sequence length
Memory:  O(n&sup2;)
Compute: O(n&sup2; &times; d)</div>

        <div class="quantization-table">
            <table>
                <thead>
                    <tr>
                        <th>From &rarr; To</th>
                        <th>Token Increase</th>
                        <th>Attention Cost Increase</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>4K &rarr; 8K</td>
                        <td>2x</td>
                        <td><strong>4x</strong></td>
                    </tr>
                    <tr>
                        <td>4K &rarr; 32K</td>
                        <td>8x</td>
                        <td><strong>64x</strong></td>
                    </tr>
                    <tr>
                        <td>4K &rarr; 128K</td>
                        <td>32x</td>
                        <td><strong>1,024x</strong></td>
                    </tr>
                    <tr>
                        <td>4K &rarr; 1M</td>
                        <td>250x</td>
                        <td><strong>62,500x</strong></td>
                    </tr>
                </tbody>
            </table>
        </div>

        <p>
            This is the fundamental reason you can't "just make context longer." The cost grows with the
            <strong>square</strong> of the length.
        </p>

        <h3>Constraint 3: KV Cache at Inference</h3>

        <p>
            At inference, you store Key and Value vectors for every past token, every layer. For a 70B model
            with GQA (8 KV heads):
        </p>

        <div class="code-example">
Per token KV cost:
  = num_layers &times; 2(K,V) &times; num_kv_heads &times; head_dim &times; 2 bytes(fp16)
  = 80 &times; 2 &times; 8 &times; 128 &times; 2
  &asymp; 327 KB per token

At different context lengths:
   4K context:  &rarr;  1.3 GB   (manageable)
  32K context:  &rarr; 10.5 GB   (one GPU)
 128K context:  &rarr;  42 GB    (needs multiple GPUs JUST for KV cache)
   1M context:  &rarr; 327 GB    (impossible on current hardware for one request)</div>

        <div class="note-block">
            <strong>Important:</strong> This is <strong>per request</strong>. A serving system handling 100 concurrent
            users at 128K context needs 4.2 TB just for KV cache &mdash; before any model weights.
        </div>

        <h3>Why You Can't "Just Extend" the Context</h3>

        <h4>Problem 1: Training Distribution Mismatch (the deepest reason)</h4>

        <p>
            The model learns attention patterns from training data. If most training documents are 2-8K tokens:
        </p>

        <ul>
            <li>It learns "the answer is usually within a few thousand tokens of the question"</li>
            <li>It learns attention heads that specialize in <strong>local</strong> patterns</li>
            <li>It <strong>never practices</strong> retrieving a fact from 50K tokens away</li>
        </ul>

        <p>
            Even if you architecturally support 128K, the model hasn't learned <strong>when and how</strong>
            to attend across that distance. This is a learned behavior problem, not an architecture problem.
        </p>

        <h4>Problem 2: Lost in the Middle (Liu et al., 2023)</h4>

        <p>
            Even models with "long context" show a U-shaped retrieval curve:
        </p>

        <div class="code-example">
Retrieval accuracy by position of relevant info:
Beginning: &block;&block;&block;&block;&block;&block;&block;&block;&block;&block;&block;&block;&block;&block;&block;&block;&block;&block;&block;&block; 95%
Middle:    &block;&block;&block;&block;&block;&block;&block;&block;             40%  &larr; catastrophic drop
End:       &block;&block;&block;&block;&block;&block;&block;&block;&block;&block;&block;&block;&block;&block;&block;&block;&block;&block;&block;  90%</div>

        <p>
            Softmax attention distributes probability mass. Over long sequences, the middle tokens get starved
            &mdash; neither benefiting from primacy nor recency bias.
        </p>

        <h4>Problem 3: Effective vs. Claimed Context</h4>

        <p>
            A model "supporting" 128K and <strong>effectively using</strong> 128K are different things:
        </p>

        <div class="code-example">
Claimed:  128K context
Needle retrieval at 128K: ~60%
Needle retrieval at 4K:   ~99%</div>

        <p>
            The spec sheet number is the architectural limit. The effective limit is much lower.
        </p>

        <h3>Context Extension Techniques &mdash; What Works and What Breaks</h3>

        <div class="quantization-table">
            <table>
                <thead>
                    <tr>
                        <th>Technique</th>
                        <th>How It Works</th>
                        <th>Trade-off</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Position Interpolation</strong></td>
                        <td>Scale positions down: pos' = pos &times; (L_train / L_target). So position 8000 &rarr; 4000 (within training range).</td>
                        <td>Reduces positional resolution. Nearby tokens look more similar. Fine distinctions blur.</td>
                    </tr>
                    <tr>
                        <td><strong>NTK-Aware Scaling</strong></td>
                        <td>Scale the RoPE frequency base instead of positions: base' = base &times; scale_factor</td>
                        <td>Better than naive interpolation. Preserves local resolution. Still needs fine-tuning on longer data.</td>
                    </tr>
                    <tr>
                        <td><strong>YaRN</strong></td>
                        <td>Combines NTK scaling with attention scaling.</td>
                        <td>Best extrapolation without full retraining. But still an approximation.</td>
                    </tr>
                    <tr>
                        <td><strong>ALiBi</strong></td>
                        <td>Linear distance bias on attention scores &mdash; no learned positions at all.</td>
                        <td>Good extrapolation, but linearly penalizes distance, limiting long-range attention by design.</td>
                    </tr>
                    <tr>
                        <td><strong>Sliding Window</strong> (Mistral)</td>
                        <td>Each token only attends to last W tokens (e.g., W=4096).</td>
                        <td>O(n&times;W) instead of O(n&sup2;). But literally cannot retrieve beyond window. Stacking layers creates indirect receptive field, but it's lossy.</td>
                    </tr>
                    <tr>
                        <td><strong>Ring Attention</strong></td>
                        <td>Split sequence across GPUs, pass KV in a ring.</td>
                        <td>Solves memory. Does NOT solve the learning problem &mdash; model still must learn long-range patterns.</td>
                    </tr>
                    <tr>
                        <td><strong>Sparse Attention</strong> (Longformer, BigBird)</td>
                        <td>Local + global + random attention patterns.</td>
                        <td>O(n) cost. But some token pairs never directly attend to each other &mdash; information is lost.</td>
                    </tr>
                    <tr>
                        <td><strong>Continue pre-training on longer sequences</strong></td>
                        <td>Actually train on progressively longer data: 4K &rarr; 16K &rarr; 64K &rarr; 128K.</td>
                        <td>Actually works. But extremely expensive (quadratic cost at each stage) and needs long documents with real dependencies.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <h3>The Fundamental Trilemma</h3>

        <div class="code-example">
        Long Context
           /\
          /  \
         /    \
        /      \
Quality -------- Efficiency</div>

        <p>Pick at most two:</p>

        <ul>
            <li><strong>Long + Quality</strong> = massive compute (full attention, long training)</li>
            <li><strong>Long + Efficient</strong> = sparse/approximate attention (loses quality)</li>
            <li><strong>Quality + Efficient</strong> = short context (what most models do best)</li>
        </ul>

        <p>
            The current frontier (Claude, GPT-4, Gemini) pushes all three using progressive training,
            GQA for KV efficiency, and massive compute budgets.
        </p>

        <hr class="section-break">

        <h2>3. What Does "Reasoning Across Long Context for Coding" Actually Mean?</h2>

        <h3>A Codebase Is Not a Document</h3>

        <p>
            When you read a novel, information flows linearly. When you read a codebase, information is
            a <strong>graph</strong>:
        </p>

        <div class="code-example">
routes/auth.py  ──imports──►  services/user.py  ──imports──►  models/user.py
      │                              │                              │
      │                              ▼                              │
      │                       utils/hashing.py                      │
      │                                                             ▼
      └──────reads───────►  config/settings.py  ◄──reads──  db/migrations/003.py
                                     ▲
                                     │
                              tests/test_auth.py</div>

        <p>
            A bug in <code>routes/auth.py</code> might exist because <code>models/user.py</code> changed a
            field name, which broke <code>services/user.py</code>, which returns <code>None</code> instead of
            raising, which causes a silent failure in the route. The <strong>relevant information is scattered
            across 5 files that have no linear relationship to each other</strong>.
        </p>

        <p>
            This is fundamentally different from "read a long document and answer questions." This is
            "hold a graph in your head and trace paths through it."
        </p>

        <h3>What "In Context" Means for an Agentic Coding Tool</h3>

        <p>
            An agentic coding tool runs an <strong>agentic loop</strong>, not a single prompt. Here's what
            actually happens when you say "fix the auth bug":
        </p>

        <div class="code-example">
Turn 1: Model reads the error → decides it needs routes/auth.py → calls Read tool
Turn 2: Sees the route calls user_service.get_user() → calls Grep to find that function
Turn 3: Reads services/user.py → sees it queries User model → calls Read on models/user.py
Turn 4: Reads the model → notices field was renamed in a migration → reads migration
Turn 5: Now has all 5 files in context → reasons across ALL of them → produces fix</div>

        <p>
            By turn 5, the context window contains the original error message, 5 source files, the grep
            results, the model's own reasoning from turns 1-4, and maybe 30-50K tokens total.
        </p>

        <div class="note-block">
            <strong>Key Point:</strong> "Reasoning across long context" means: at turn 5, the model is
            simultaneously attending to information from ALL of these sources to produce a coherent fix. It's
            not just looking at the file it's currently editing &mdash; it's cross-referencing the migration
            from turn 4 with the model definition from turn 3 with the service call from turn 2.
        </div>

        <h3>What "Reasoning Across" Actually Requires</h3>

        <p>There are specific cognitive operations that "reasoning across context" implies:</p>

        <h4>Cross-File Causal Tracing</h4>

        <pre><code class="language-python"># File A (in context at position ~2K)
def get_user(id):
    return db.query(User).filter(User.user_id == id).first()  # returns None if not found

# File B (in context at position ~15K)
def login(request):
    user = get_user(request.id)
    token = generate_token(user.email)  # ← crashes: NoneType has no attribute 'email'</code></pre>

        <p>
            The model must connect a <code>None</code> return in one file to an unguarded attribute access
            in another file. These two code fragments might be <strong>13,000 tokens apart</strong> in the
            context. The model's attention mechanism must literally assign high attention weight from the
            <code>user.email</code> token to the <code>return ... .first()</code> token across that gap.
        </p>

        <p>
            <strong>This is what "lost in the middle" kills.</strong> A weaker model sees File B, generates
            a fix like <code>if user is None: return error</code> &mdash; which is correct but shallow. A
            model reasoning across full context also notices that <code>get_user</code> should probably raise
            <code>UserNotFoundError</code> instead of returning <code>None</code>, because 6 other call sites
            (also in context from earlier grep results) all assume it returns a valid user.
        </p>

        <h4>Pattern Recognition Across the Codebase</h4>

        <p>After reading 8-10 files, the model should recognize:</p>

        <ul>
            <li>"This codebase uses the repository pattern"</li>
            <li>"Errors are custom exceptions caught by middleware, not return codes"</li>
            <li>"Every endpoint has a corresponding Pydantic schema in <code>schemas/</code>"</li>
            <li>"Tests use factory_boy fixtures, not raw object creation"</li>
        </ul>

        <p>
            This is <strong>not</strong> stored in any single file. It emerges from reasoning across the
            <strong>aggregate</strong> context. When the model then generates new code, it should follow
            these patterns &mdash; not because it was told to, but because it <strong>inferred</strong>
            them from the context.
        </p>

        <p>
            A model that can't reason across long context will generate code that is locally correct but
            <strong>stylistically alien</strong> to the codebase.
        </p>

        <h4>Consistency Maintenance Across Edits</h4>

        <p>
            Say the model needs to rename <code>user_id</code> to <code>account_id</code> across the codebase. It:
        </p>

        <ul>
            <li>Finds 23 occurrences across 11 files</li>
            <li>Must change ALL 23, not 20</li>
            <li>Must change them in type definitions, function signatures, database queries, test assertions, API schemas, and migration files</li>
            <li>Must NOT change the string <code>"user_id"</code> in a comment explaining the old schema</li>
        </ul>

        <p>
            This requires <strong>holding all 23 occurrences in working memory simultaneously</strong> and
            reasoning about each one's semantic role. Miss one and you've introduced a bug. Change a wrong
            one and you've introduced a different bug.
        </p>

        <h4>Multi-Step Plan Coherence</h4>

        <div class="code-example">
Step 1: Add new column to User model        → produces migration
Step 2: Update UserSchema to include field   → must match the column type from step 1
Step 3: Update create_user service           → must use the schema from step 2
Step 4: Update API endpoint                  → must match the service signature from step 3
Step 5: Add test                             → must test the endpoint from step 4 with
                                                the schema from step 2 using the model from step 1</div>

        <p>
            By step 5, the model is writing test code that must be <strong>simultaneously consistent with
            decisions made in steps 1-4</strong>, all of which are in the context from earlier turns. If the
            model "forgets" that it used <code>account_id</code> in step 1 and writes <code>user_id</code>
            in the step 5 test, <strong>the entire chain breaks</strong>.
        </p>

        <h3>The Difference Between "Holds" and "Reasons Across"</h3>

        <p>
            This is the crux. Many models support 128K tokens. The question is what happens to information
            at various positions:
        </p>

        <div class="code-example">
Model that HOLDS long context:
  ✓ Can regurgitate text from position 60K if asked "what was in file X?"
  ✗ Does NOT spontaneously use info from position 60K when generating code at position 120K

Model that REASONS ACROSS long context:
  ✓ While generating code at position 120K, attention heads actively pull
    information from position 60K because it's relevant
  ✓ Does this without being explicitly told "look at file X"
  ✓ Does this even when the connection is implicit (same variable name,
    compatible type signature, related business logic)</div>

        <p>
            The difference is whether the model's <strong>attention patterns during generation</strong>
            actually reach back and meaningfully integrate distant information, versus just having it
            present but functionally ignored.
        </p>

        <h3>Why This Is So Hard (The Non-Obvious Part)</h3>

        <p>
            The real difficulty isn't memory &mdash; it's <strong>relevance detection at scale</strong>.
        </p>

        <p>
            At 100K tokens of context, there might be 500 function definitions, 200 class attributes,
            50 config values, and 100 test assertions. When generating one line of code, maybe 3-4 of
            those are relevant. The model must:
        </p>

        <ol>
            <li><strong>Not attend to</strong> the 796 irrelevant items (noise suppression)</li>
            <li><strong>Strongly attend to</strong> the 4 relevant items (signal detection)</li>
            <li>Do this <strong>for every token it generates</strong></li>
        </ol>

        <p>
            This is a needle-in-a-haystack problem <strong>at every single generation step</strong>, not
            just once. And the "needle" changes for every token &mdash; the relevant context for choosing
            a function name is different from the relevant context for choosing its arguments.
        </p>

        <div class="code-example">
Generating: token = generate_token(user.?????)

To decide this next token, the model must:
- Recall User model definition (position 8K): field is called "email_address" not "email"
- Recall generate_token signature (position 22K): first param is type str
- Recall project convention (positions 5K, 12K, 31K): always uses model attributes directly
- Ignore 99.5% of everything else in context

→ Output: "email_address"</div>

        <h3>Summary: What "Reasons Across Long Context for Coding" Claims</h3>

        <ol>
            <li><strong>The agentic loop</strong> gathers the right files into context (tool use + search strategy)</li>
            <li><strong>The model</strong> attends to information across the full accumulated context &mdash; not just the most recent file</li>
            <li><strong>Cross-references</strong> are made between distant parts of the context without explicit prompting</li>
            <li><strong>Generated code</strong> is consistent with patterns, types, names, and conventions observed anywhere in the context, not just locally</li>
            <li><strong>Multi-step edits</strong> maintain coherence from first change to last</li>
            <li><strong>Causal chains</strong> spanning multiple files are traced correctly</li>
        </ol>

        <div class="note-block">
            <strong>Bottom Line:</strong> It's the combination of the agentic scaffolding (getting the right info
            into context) and the model's actual attention quality over that context (using the info once it's there)
            that makes it work.
        </div>

        <h3>Implications for Fine-Tuning & Evaluation</h3>

        <ul>
            <li><strong>Evaluation is hard.</strong> You can't test "long context reasoning for code" with a simple needle-in-a-haystack test. You need:
                <ul>
                    <li>Multi-file edit consistency benchmarks</li>
                    <li>Cross-file bug localization tasks</li>
                    <li>"Apply this pattern codebase-wide" tasks</li>
                    <li>Multi-step refactoring that must maintain correctness at each step</li>
                </ul>
            </li>
            <li><strong>Post-training (RLHF/RL) matters</strong> &mdash; supervised fine-tuning teaches the model what good code looks like locally, but RL teaches it to <strong>maintain coherence across the full context</strong>.</li>
        </ul>

        <hr class="section-break">

        <h2>4. Why 200K Context and Not 1M? &mdash; And Why 100K Is Enough</h2>

        <h3>The Problem: Bigger Context &ne; Better Reasoning</h3>

        <div class="quantization-table">
            <table>
                <thead>
                    <tr>
                        <th>Context Length</th>
                        <th>Needle Retrieval</th>
                        <th>Reasoning Quality</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>4K</td>
                        <td>~99%</td>
                        <td>excellent</td>
                    </tr>
                    <tr>
                        <td>32K</td>
                        <td>~97%</td>
                        <td>very good</td>
                    </tr>
                    <tr>
                        <td>100K</td>
                        <td>~90%</td>
                        <td>good</td>
                    </tr>
                    <tr>
                        <td>200K</td>
                        <td>~85%</td>
                        <td>acceptable</td>
                    </tr>
                    <tr>
                        <td>500K</td>
                        <td>~60%</td>
                        <td>degraded</td>
                    </tr>
                    <tr>
                        <td>1M</td>
                        <td>~40%</td>
                        <td>significantly degraded</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <p>
            Going from 200K to 1M doesn't give 5x useful context &mdash; it gives ~800K tokens the model
            <strong>barely attends to</strong>. Three forces make larger context impractical:
        </p>

        <p>
            <strong>Cost:</strong> At 1M context, a 30-turn coding session costs ~$36 vs ~$7 at 200K (5x).
            Most of those extra tokens are noise.
        </p>

        <p>
            <strong>Latency:</strong> Attention is O(n&sup2;). Doubling context = 4x compute. 200K&rarr;1M =
            <strong>25x attention cost</strong>. Prefill jumps from ~3-5s to ~15-30s per turn, making an
            agentic loop unusable.
        </p>

        <p>
            <strong>Training:</strong> Training on 1M-length sequences costs 62,500x more per sample than 4K.
            And finding millions of high-quality 1M-token documents with genuine long-range dependencies is
            nearly impossible.
        </p>

        <h3>The Solution: Smart Agents Beat Big Context</h3>

        <h4>Problem: Current agents waste most of their context</h4>

        <div class="code-example">
Typical 56K token session breakdown:
  System prompt + instructions:       3K   ✓ needed
  Relevant source files:             7.5K  ✓ needed
  Grep results:                       3K   ~60% useful
  Irrelevant file (wrong lead):      1.5K  ✗ wasted
  Model's stale reasoning history:   12K   ✗ redundant after acting on it
  Tool call formatting overhead:      4K   ✗ pure overhead
  Old conversation turns (verbatim): 25K   ✗ mostly stale
  ─────────────────────────────────────────
  Total:                             56K
  Actually needed for the fix:       ~12K  ← only 21% is signal</div>

        <h4>Solution 1: Surgical Retrieval &mdash; Read functions, not files</h4>

        <div class="code-example">
Dumb:   Read all of user_service.py (2000 lines)      → ~8K tokens
Smart:  Read lines 145-178 (the one relevant function) → ~200 tokens
                                                         40x reduction</div>

        <p>
            <strong>Why it works:</strong> The agent already knows (from the stack trace or grep) which function
            matters. Reading the whole file dumps 1800 lines of noise into context that competes for attention
            with the 30 lines that matter.
        </p>

        <h4>Solution 2: Context Eviction &mdash; Proactively discard stale information</h4>

        <div class="code-example">
Turn 3:  Read models/user.py to understand schema     → 800 tokens in context
Turn 5:  Already used that info to write the fix

Current: Keep the full file verbatim forever           → 800 tokens wasted
Smart:   Compress to "User model: id, email_address, created_at"  → 20 tokens
                                                         40x reduction</div>

        <p>
            <strong>Why it works:</strong> After you've acted on information, you rarely need it verbatim. A
            summary preserves the decision-relevant facts while freeing context for new information. Current
            agents only compress when hitting limits &mdash; smart agents compress proactively.
        </p>

        <h4>Solution 3: Structured Memory &mdash; Notes over verbatim copies</h4>

        <div class="code-example">
Current (raw context):
  Full file stored in conversation history  → 2000 tokens per file
  Degrades as it drifts further from attention window

Optimized (structured scratchpad):
  {
    "file": "models/user.py",
    "facts": ["email_address: str", "uses SQLAlchemy", "has created_at"],
    "re_read_lines": "145-150"    ← pointer to re-read if needed
  }
  → 30 tokens, and exact lines are one tool call away</div>

        <p>
            <strong>Why it works:</strong> Gives the agent an external "notebook" instead of relying on attention
            over raw tokens. The model decides what's important, writes it down concisely, and can re-fetch
            details on demand.
        </p>

        <h4>Solution 4: Plan Before Reading &mdash; Avoid wrong leads entirely</h4>

        <div class="code-example">
Dumb:   "auth broken" → reads auth.py, user.py, session.py, middleware.py,
        config.py, database.py → discovers only auth.py + user.py mattered
        Cost: 6 file reads × ~2K = 12K tokens, 4 were wasted

Smart:  "auth broken" → reads stack trace (200 tokens) → thinks "crash is in
        login() calling get_user(), need those two files only" → reads 2 files
        Cost: 2 file reads × ~2K = 4K tokens, 0 wasted</div>

        <p>
            <strong>Why it works:</strong> 200 tokens of planning saves 8K tokens of unnecessary reads. The model
            spends attention budget thinking about what to retrieve rather than dumping everything and hoping
            attention sorts it out.
        </p>

        <h3>The Core Principle</h3>

        <div class="code-example">
Unlimited context → lazy agent → dump everything → hope attention finds signal
                                                    (it often doesn't)

Tight context     → disciplined agent → precise retrieval → every token earns its place
                                                             (attention concentrated on signal)</div>

        <div class="note-block">
            <strong>Key Insight:</strong> A <strong>100K agent that's selective will outperform a 1M agent
            that's sloppy</strong> because:
            <ul>
                <li>Higher signal-to-noise ratio &rarr; attention works better on what's there</li>
                <li>Less "lost in the middle" because there's less middle</li>
                <li>Faster per-turn &rarr; more iterations possible &rarr; better refinement</li>
                <li>Cheaper per turn &rarr; can afford longer sessions</li>
            </ul>
            This mirrors <strong>human working memory</strong>: we hold ~7 items. That constraint forces us to
            organize, prioritize, and abstract &mdash; producing better reasoning than trying to hold everything at once.
        </div>

        <h3>Where the Field Is Heading</h3>

        <div class="code-example">
Phase 1 (2023): "Make context bigger"     → brute force, GPT-4 128K, Gemini 1M
Phase 2 (2024): "Make context smarter"    → better attention training, YaRN, RoPE scaling
Phase 3 (2025+): "Make agents leaner"     → use less context more effectively
                  ├── Surgical retrieval   (read functions, not files)
                  ├── Active eviction      (discard stale context proactively)
                  ├── Structured memory    (notes > verbatim copies)
                  ├── Plan-then-retrieve   (think before reading)
                  └── Parallel targeted reads (speculative tool use)</div>

        <div class="note-block">
            <strong>The Endgame:</strong> An agent that navigates a million-file codebase using a 50K context
            window, because it knows exactly where to look and what to remember. The next breakthrough won't
            be "2M context" &mdash; it'll be "same quality at 64K with 10x less cost and 5x less latency."
        </div>

        <hr class="section-break">

        <h2>5. How Agentic Coding Tools Achieve Long-Context Reasoning</h2>

        <p>
            No vector database. No knowledge graph. No external memory. Just a flat conversation where the
            <strong>model is simultaneously the retriever, the planner, and the reasoner</strong>.
        </p>

        <h3>The Architecture: Everything Is a Conversation</h3>

        <div class="code-example">
Msg 1:  [system]     System prompt + project instructions         (~3K tokens)
Msg 2:  [user]       "Fix the auth bug"                          (~50 tokens)
Msg 3:  [assistant]  Thinking + tool_call: Grep("auth error")    (~200 tokens)
Msg 4:  [tool]       Grep results: routes/auth.py:47             (~500 tokens)
Msg 5:  [assistant]  Thinking + tool_call: Read("routes/auth.py")
Msg 6:  [tool]       File contents                                (~2K tokens)
Msg 7:  [assistant]  Thinking + tool_call: Read("services/user.py")
Msg 8:  [tool]       File contents                                (~1.5K tokens)
Msg 9:  [assistant]  Thinking + tool_call: Edit(...)
Msg 10: [tool]       Edit successful
Msg 11: [assistant]  "Fixed. Here's what I changed..."</div>

        <p>
            At message 11, the model sees <strong>ALL of messages 1-10</strong>. "Long context reasoning" =
            at this point, attention reaches back across every previous message simultaneously to produce a
            coherent fix.
        </p>

        <h3>Mechanism 1: The Model IS the Retriever</h3>

        <div class="code-example">
RAG pipeline:
  Query → Embedding model → Vector search → Top-K chunks → LLM generates
  Problem: Embedding model and LLM are DIFFERENT models.
           Retriever doesn't know what the generator will need.
           Selection is based on surface similarity, not causal reasoning.

Agentic approach:
  Question → Model thinks "crash is in login(), which calls get_user()" → Grep
           → Model reads result, thinks "defined in user.py, need to check return type" → Read
           → The SAME model choosing what to retrieve will use what it retrieves.</div>

        <p>The critical difference:</p>

        <div class="code-example">
RAG retriever:     "This text is 0.87 similar to the query"     (statistical)
Agentic retriever: "The crash is in login() calling get_user(),
                    I need to read where get_user is defined"   (causal reasoning)</div>

        <p>
            No embedding approximation. No similarity threshold. The model reasons about code dependencies
            and retrieves based on <strong>understanding</strong>, not pattern matching.
        </p>

        <h3>Mechanism 2: Iterative Context Building</h3>

        <p>
            The agent doesn't load everything at once. Understanding is built turn by turn:
        </p>

        <div class="code-example">
Turn 1: Read error       → "login endpoint crashes with NoneType"
Turn 2: Read auth.py     → "user.email fails because user is None"
Turn 3: Grep get_user    → "get_user is in services/user.py, returns .first()"
Turn 4: Read user.py     → ".first() returns None when no match, no error raised"
Turn 5: Read tests       → "tests expect UserNotFoundError — but it's never raised"</div>

        <p>
            Each turn refines understanding. By turn 5, the model has a complete causal chain in context AND
            has been building its mental model incrementally. This is fundamentally better than dumping 5 files
            cold &mdash; the <strong>order of discovery</strong> helps the model organize information.
        </p>

        <h3>Mechanism 3: Automatic Context Compression</h3>

        <p>When context approaches the limit, older messages get summarized:</p>

        <div class="code-example">
Before (approaching limit):
  Messages 1-30: Full verbatim content                    (~180K tokens)

After compression:
  Messages 1-15: Summarized by a fast model               (~8K tokens)
    "Investigated auth bug. get_user() returns None instead of raising.
     Fixed routes/auth.py and services/user.py. User asked to update tests."
  Messages 16-30: Full verbatim content                    (~90K tokens)
  Total: ~98K — back under budget</div>

        <p>
            This mirrors what attention does naturally (recent = high attention, old = low attention), but
            makes it <strong>explicit and honest</strong> instead of pretending the model attends well to
            180K tokens.
        </p>

        <h3>Mechanism 4: Ground Truth Over Approximation</h3>

        <p>Every tool result is <strong>actual file content</strong>, not an embedding or summary:</p>

        <div class="code-example">
Vector RAG:     Retrieves chunk that's 0.89 similar. Maybe correct. Maybe stale index.
Claude Code:    Reads the actual file right now. Always current. Exact syntax.</div>

        <p>
            For code, this matters enormously &mdash; a single wrong character (<code>.email</code> vs
            <code>.email_address</code>) breaks everything. Embeddings can't distinguish these; direct
            file reads can.
        </p>

        <h3>What Attention Does During Generation</h3>

        <p>When generating an edit, the model's attention simultaneously reaches:</p>

        <div class="code-example">
Position ~500    (system prompt):   "write safe, secure code"        → error handling style
Position ~1200   (user message):    "fix the auth bug"               → intent
Position ~5000   (grep results):    "get_user at user.py:152"        → function location
Position ~8000   (user.py source):  "return ...first()"              → knows it returns Optional
Position ~12000  (auth.py source):  "user.email"                     → the crash point
Position ~14000  (test file):       "assert raises(UserNotFoundError)"→ expected behavior

All positions attended to simultaneously at every generated token.</div>

        <p>
            A model "good at long context" = attention heads at position 15000 successfully assign high
            weight to relevant tokens at positions 500, 5000, 8000, 12000, 14000 &mdash; without being
            told which positions to look at.
        </p>

        <h3>The Design Tradeoff</h3>

        <div class="comparison-table">
            <div class="comparison-column">
                <h5>What this approach gives up</h5>
                <ul>
                    <li>No persistence across sessions (re-explores each time)</li>
                    <li>No pre-indexed codebase (must search fresh)</li>
                    <li>Each tool call has latency (not instant retrieval)</li>
                </ul>
            </div>
            <div class="comparison-column">
                <h5>What this approach gains</h5>
                <ul>
                    <li>Zero setup &mdash; works on any codebase immediately</li>
                    <li>Always-fresh reads &mdash; no stale index</li>
                    <li>100% ground truth &mdash; actual file contents, not approximations</li>
                    <li>Retrieval guided by reasoning, not statistical similarity</li>
                    <li>Simple architecture &mdash; no external systems to maintain</li>
                </ul>
            </div>
        </div>

        <h3>Summary</h3>

        <div class="code-example">
How agentic coding tools achieve long-context reasoning:

1. ARCHITECTURE:   Flat conversation. No hidden memory. No vector DB.
2. RETRIEVAL:      The model IS the retriever — reasons about what to read.
3. BUILDING:       Context built iteratively, each turn refining understanding.
4. COMPRESSION:    Old context summarized when approaching limits.
5. GROUND TRUTH:   Tool results are actual file contents, not approximations.
6. ATTENTION:      At generation, model attends across ALL accumulated context.

Works because the model is good enough to:
  plan retrieval → execute it → reason across all results simultaneously</div>

        <hr class="section-break">

        <h2>Key Concepts Reference</h2>

        <div class="quantization-table">
            <table>
                <thead>
                    <tr>
                        <th>Concept</th>
                        <th>One-Line Summary</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Long-horizon reasoning</td>
                        <td>Planning and executing across many dependent steps without error compounding</td>
                    </tr>
                    <tr>
                        <td>Lost in the middle</td>
                        <td>Models attend to start/end of context but lose information in the middle</td>
                    </tr>
                    <tr>
                        <td>Effective vs. claimed context</td>
                        <td>Architectural support &ne; actual retrieval/reasoning ability at that length</td>
                    </tr>
                    <tr>
                        <td>Cross-file causal tracing</td>
                        <td>Connecting cause in file A to effect in file B across large token distances</td>
                    </tr>
                    <tr>
                        <td>Holds vs. reasons across</td>
                        <td>Storing tokens &ne; actively using them during generation</td>
                    </tr>
                    <tr>
                        <td>Relevance detection at scale</td>
                        <td>Picking the 4 relevant items out of 800 at every generation step</td>
                    </tr>
                    <tr>
                        <td>Process reward models</td>
                        <td>Rewarding each reasoning step, not just the final answer</td>
                    </tr>
                    <tr>
                        <td>Agentic context accumulation</td>
                        <td>Iteratively gathering the right information into context via tool use</td>
                    </tr>
                    <tr>
                        <td>Surgical retrieval</td>
                        <td>Reading only the relevant function/lines instead of entire files</td>
                    </tr>
                    <tr>
                        <td>Context eviction</td>
                        <td>Proactively discarding stale information to keep context tight</td>
                    </tr>
                    <tr>
                        <td>Structured memory</td>
                        <td>Storing extracted facts + line pointers instead of verbatim file content</td>
                    </tr>
                    <tr>
                        <td>Plan-then-retrieve</td>
                        <td>Spending tokens thinking about what to read before reading anything</td>
                    </tr>
                    <tr>
                        <td>Signal-to-noise ratio</td>
                        <td>The % of context tokens that actually contribute to the current generation</td>
                    </tr>
                    <tr>
                        <td>O(n&sup2;) attention cost</td>
                        <td>Why doubling context = 4x compute, making 1M context 25x costlier than 200K</td>
                    </tr>
                    <tr>
                        <td>RoPE</td>
                        <td>Encodes position as rotation angle; works within training range, degrades outside it</td>
                    </tr>
                    <tr>
                        <td>Position interpolation</td>
                        <td>Scale positions down to fit trained range; trades resolution for extrapolation</td>
                    </tr>
                    <tr>
                        <td>YaRN / NTK scaling</td>
                        <td>Scale RoPE frequency base instead of positions; better local resolution</td>
                    </tr>
                    <tr>
                        <td>KV cache</td>
                        <td>Stored Key/Value vectors per token per layer; ~327 KB/token for 70B model</td>
                    </tr>
                    <tr>
                        <td>GQA</td>
                        <td>Share KV heads across query heads to reduce KV cache size</td>
                    </tr>
                    <tr>
                        <td>Training distribution mismatch</td>
                        <td>Model never practiced attending to position 50K if trained on 4K docs</td>
                    </tr>
                    <tr>
                        <td>Context trilemma</td>
                        <td>Long + Quality + Efficient &mdash; pick at most two</td>
                    </tr>
                    <tr>
                        <td>Model-as-retriever</td>
                        <td>The generating model also decides what to retrieve, replacing separate search</td>
                    </tr>
                    <tr>
                        <td>Iterative context building</td>
                        <td>Understanding built turn-by-turn; order of discovery helps organize information</td>
                    </tr>
                    <tr>
                        <td>Automatic context compression</td>
                        <td>Summarize old turns when approaching limits; honest version of attention decay</td>
                    </tr>
                </tbody>
            </table>
        </div>

    </article>

    <footer>
        <p>&copy; 2026 Akhil Shekkari &middot; <a href="https://github.com/shekkari1999">GitHub</a></p>
    </footer>

    <script>
        Prism.highlightAll();
    </script>
</body>
</html>
